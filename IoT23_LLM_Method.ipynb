{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Use network packets to finetune the model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Step1: Preprocess the dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"### Network packet information:\n",
    "{information}\n",
    "\n",
    "### The type of this netowork packet is:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/iot23_combined.csv\"\n",
    "# df = pd.read_csv(filepath, nrows=1000)\n",
    "df = pd.read_csv(filepath)\n",
    "# df = df.drop_duplicates()\n",
    "sampled_df = pd.DataFrame()  # Create an empty DataFrame，to store the filter dataset \n",
    "for label_value in [\"Okiru\", \"DDoS\"]: #,   \"PartOfAHorizontalPortScan\", \"Benign\", \"C&C\", \"Okiru\", \"DDoS\"\n",
    "    type_subset = df[df['label'] == label_value].sample(n=3200, random_state=1)\n",
    "    sampled_df = pd.concat([sampled_df, type_subset])\n",
    "\n",
    "\n",
    "del sampled_df['Unnamed: 0']\n",
    "X = sampled_df[['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'proto_icmp', 'proto_tcp', 'proto_udp', 'conn_state_OTH', 'conn_state_REJ', 'conn_state_RSTO', 'conn_state_RSTOS0', 'conn_state_RSTR', 'conn_state_RSTRH', 'conn_state_S0', 'conn_state_S1', 'conn_state_S2', 'conn_state_S3', 'conn_state_SF', 'conn_state_SH', 'conn_state_SHR']]\n",
    "Y = sampled_df['label']\n",
    "data_dict = sampled_df.to_dict(orient='records')\n",
    "data_dict\n",
    "# sampled_df.to_csv('sampled_iot_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_unique = sampled_df.drop_duplicates()\n",
    "# sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duration', 'payload bytes the originator sent', 'payload bytes the responder sent', 'missed bytes', 'number of packets that the originator sent', 'originator ip bytes', 'responder pkts', 'responder ip bytes', 'label', 'protocol icmp', 'protocol tcp', 'protocol udp', 'connection state OTH', 'connection state REJ', 'connection state RSTO', 'connection state RSTOS0', 'connection state RSTR', 'connection state RSTRH', 'connection state S0', 'connection state S1', 'connection state S2', 'connection state S3', 'connection state SF', 'connection state SH', 'connection state SHR']\n"
     ]
    }
   ],
   "source": [
    "#modify the dataframe column name \n",
    "# new_sampled_df=sampled_df.rename(columns={\"duration\":\"duration\",\"orig_bytes\": \"payload_bytes_the_originator_sent\", \"resp_bytes\": \"payload_bytes_the_responder_sent\",\"orig_pkts\":\"number_of_packets_that_the_originator_sent\",\n",
    "#                                           \"orig_ip_bytes\":\"originator_ip_bytes\",\"resp_pkts\":\"responder_pkts\",\n",
    "#                                           \"resp_ip_bytes\":\"responder_ip_bytes\",\"proto_icmp\":\"protocol_icmp\",\n",
    "#                                           \"proto_tcp\":\"protocol_tcp\",\"proto_udp\":\"protocol_udp\",\n",
    "#                                           \"conn_state_OTH\": \"connection_state_OTH\", \"conn_state_REJ\":\"connection_state_REJ\", \n",
    "#                                           \"conn_state_RSTO\":\"connection_state_RSTO\", \"conn_state_RSTOS0\":\"connection_state_RSTOS0\", \n",
    "#                                           \"conn_state_RSTR\":\"connection_state_RSTR\", \"conn_state_RSTRH\":\"connection_state_RSTRH\", \n",
    "#                                           \"conn_state_S0\":\"connection_state_S0\", \"conn_state_S1\":\"connection_state_S1\", \n",
    "#                                           \"conn_state_S2\":\"connection_state_S2\", \"conn_state_S3\":\"connection_state_S3\", \n",
    "#                                           \"conn_state_SF\":\"connection_state_SF\", \"conn_state_SH\":\"connection_state_SH\", \n",
    "#                                           \"conn_state_SHR\":\"connection_state_SHR\"\n",
    "#                                           })\n",
    "new_sampled_df2=sampled_df.rename(columns={\"duration\":\"duration\",\"orig_bytes\": \"payload bytes the originator sent\", \n",
    "                                           \"resp_bytes\": \"payload bytes the responder sent\",\n",
    "                                           \"missed_bytes\": \"missed bytes\",\n",
    "                                           \"orig_pkts\":\"number of packets that the originator sent\",\n",
    "                                          \"orig_ip_bytes\":\"originator ip bytes\",\"resp_pkts\":\"responder pkts\",\n",
    "                                          \"resp_ip_bytes\":\"responder ip bytes\",\"proto_icmp\":\"protocol icmp\",\n",
    "                                          \"proto_tcp\":\"protocol tcp\",\"proto_udp\":\"protocol udp\",\n",
    "                                          \"conn_state_OTH\": \"connection state OTH\", \"conn_state_REJ\":\"connection state REJ\", \n",
    "                                          \"conn_state_RSTO\":\"connection state RSTO\", \"conn_state_RSTOS0\":\"connection state RSTOS0\", \n",
    "                                          \"conn_state_RSTR\":\"connection state RSTR\", \"conn_state_RSTRH\":\"connection state RSTRH\", \n",
    "                                          \"conn_state_S0\":\"connection state S0\", \"conn_state_S1\":\"connection state S1\", \n",
    "                                          \"conn_state_S2\":\"connection state S2\", \"conn_state_S3\":\"connection state S3\", \n",
    "                                          \"conn_state_SF\":\"connection state SF\", \"conn_state_SH\":\"connection state SH\", \n",
    "                                          \"conn_state_SHR\":\"connection state SHR\"\n",
    "                                          })\n",
    "\n",
    "\n",
    "\n",
    "new_sampled_df2.to_csv(\"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/new_sampled2_df_6400_Okiru_DDoS_original.csv\",index=False)\n",
    "colmn=pd.read_csv(\"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/new_sampled2_df_6000_Okiru_DDoS_original.csv\")\n",
    "\n",
    "print([column for column in colmn])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#直接读取csv文件\n",
    "filepath = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/experiment7_combined_data.csv\"\n",
    "sampled_df = pd.read_csv(filepath)\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PartOfAHorizontalPortScan    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_unique = sampled_df.drop_duplicates()\n",
    "data_unique['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">Data prepare</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data prepare,合并train data，取得validation和test data\n",
    "filepath = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/experiment8_16000_combined_data.csv\"\n",
    "sampled_df = pd.read_csv(filepath) \n",
    "df_shuffled = sampled_df.sample(frac=1)  #打乱这个数据\n",
    "folderpath=\"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8\"\n",
    "\n",
    "# 划分数据集\n",
    "train_data = df_shuffled[:5000]\n",
    "validation_data = df_shuffled[5000:6000]\n",
    "test_data = df_shuffled[6000:]\n",
    "\n",
    "# 保存 train data 和 validation data\n",
    "train_data.to_csv(folderpath+'/Experiment8_5000_train_data.csv', index=False)\n",
    "validation_data.to_csv(folderpath+'/Experiment8_1000validation_data.csv', index=False)\n",
    "\n",
    "# 分割 test data 并保存\n",
    "test_data_splits = np.array_split(test_data, 10)\n",
    "for i, split in enumerate(test_data_splits):\n",
    "    split.to_csv(folderpath+f'/Experiment8_test_data_{i+1}.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer csv file to jsonl\n",
    "\n",
    "#生成josn list\n",
    "def csv_transfer_jsonal(sampled_data_csv,jsonal_file_path):\n",
    "    sampled_df_data = pd.read_csv(sampled_data_csv)\n",
    "    json_list_data = []\n",
    "    for index, row in sampled_df_data.iterrows():\n",
    "        # X = row[['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'proto_icmp', 'proto_tcp', 'proto_udp', 'conn_state_OTH', 'conn_state_REJ', 'conn_state_RSTO', 'conn_state_RSTOS0', 'conn_state_RSTR', 'conn_state_RSTRH', 'conn_state_S0', 'conn_state_S1', 'conn_state_S2', 'conn_state_S3', 'conn_state_SF', 'conn_state_SH', 'conn_state_SHR']] #original label\n",
    "        # X = row[['duration', 'payload bytes_the_originator_sent', 'payload_bytes_the_responder_sent', 'missed_bytes', 'number_of_packets_that_the_originator_sent', 'originator_ip_bytes', 'responder_pkts', 'responder_ip_bytes', 'protocol_icmp', 'protocol_tcp', 'protocol_udp', 'connection_state_OTH', 'connection_state_REJ', 'connection_state_RSTO', 'connection_state_RSTOS0', 'connection_state_RSTR', 'connection_state_RSTRH', 'connection_state_S0', 'connection_state_S1', 'connection_state_S2', 'connection_state_S3', 'connection_state_SF', 'connection_state_SH', 'connection_state_SHR']] #My label\n",
    "        X = row[['duration', 'payload bytes the originator sent', 'payload bytes the responder sent', 'missed bytes', 'number of packets that the originator sent', 'originator ip bytes', 'responder pkts', 'responder ip bytes', 'protocol icmp', 'protocol tcp', 'protocol udp', 'connection state OTH', 'connection state REJ', 'connection state RSTO', 'connection state RSTOS0', 'connection state RSTR', 'connection state RSTRH', 'connection state S0', 'connection state S1', 'connection state S2', 'connection state S3', 'connection state SF', 'connection state SH', 'connection state SHR']] #My label\n",
    "        Y = row['label']\n",
    "        X_dict = X.to_dict()    \n",
    "\n",
    "        json_object_gpt_turbo = {\"messages\": [{\"role\": \"system\", \"content\": \"You are a good network packet analyzer\\ Please analyze the network packet information provided by the user and determine its type\\ The possible types are 'PartOfAHorizontalPortScan', 'Okiru', 'Benign', 'DDoS', 'C&C'\\ Provide only the most likely type from these options as final output\\ Only output this type and nothing else\\ Double-check your answer to ensure accuracy before outputting\"}, {\"role\": \"user\", \"content\": prompt_template.format(information=str(X_dict))}, {\"role\": \"assistant\", \"content\":Y }]}\n",
    "        json_object_babbage= {\"prompt\": prompt_template.format(information=str(X_dict)), \"completion\": Y}\n",
    "        json_list_data.append(json_object_gpt_turbo)\n",
    "    # new method: store train and test data to jsonal file\n",
    "    # Write train data to file\n",
    "    with open(jsonal_file_path, \"w\") as train_file:\n",
    "        for json_object in json_list_data:\n",
    "            line = json.dumps(json_object) + \"\\n\"\n",
    "            train_file.write(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_transfer_jsonal(\"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/Experiment8_test_data_10.csv\",\n",
    "                    \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/Experiment8_test_data_10.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>payload bytes the originator sent</th>\n",
       "      <th>payload bytes the responder sent</th>\n",
       "      <th>missed bytes</th>\n",
       "      <th>number of packets that the originator sent</th>\n",
       "      <th>originator ip bytes</th>\n",
       "      <th>responder pkts</th>\n",
       "      <th>responder ip bytes</th>\n",
       "      <th>label</th>\n",
       "      <th>protocol icmp</th>\n",
       "      <th>...</th>\n",
       "      <th>connection state RSTOS0</th>\n",
       "      <th>connection state RSTR</th>\n",
       "      <th>connection state RSTRH</th>\n",
       "      <th>connection state S0</th>\n",
       "      <th>connection state S1</th>\n",
       "      <th>connection state S2</th>\n",
       "      <th>connection state S3</th>\n",
       "      <th>connection state SF</th>\n",
       "      <th>connection state SH</th>\n",
       "      <th>connection state SHR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.002965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>PartOfAHorizontalPortScan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C&amp;C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C&amp;C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.037150</td>\n",
       "      <td>0.031855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>C&amp;C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C&amp;C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>C&amp;C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      duration  payload bytes the originator sent  \\\n",
       "0     0.000044                           0.000609   \n",
       "1     0.000044                           0.000609   \n",
       "2     0.000000                           0.000000   \n",
       "3     0.000000                           0.000000   \n",
       "4     0.000044                           0.000609   \n",
       "...        ...                                ...   \n",
       "4995  0.000183                           0.000609   \n",
       "4996  0.000044                           0.000609   \n",
       "4997  0.000089                           0.037150   \n",
       "4998  0.000184                           0.000609   \n",
       "4999  0.000044                           0.000609   \n",
       "\n",
       "      payload bytes the responder sent  missed bytes  \\\n",
       "0                             0.000255           0.0   \n",
       "1                             0.000255           0.0   \n",
       "2                             0.000000           0.0   \n",
       "3                             0.000000           0.0   \n",
       "4                             0.000255           0.0   \n",
       "...                                ...           ...   \n",
       "4995                          0.000255           0.0   \n",
       "4996                          0.000255           0.0   \n",
       "4997                          0.031855           0.0   \n",
       "4998                          0.000255           0.0   \n",
       "4999                          0.000255           0.0   \n",
       "\n",
       "      number of packets that the originator sent  originator ip bytes  \\\n",
       "0                                       0.001342             0.001482   \n",
       "1                                       0.001342             0.000988   \n",
       "2                                       0.002685             0.002965   \n",
       "3                                       0.005369             0.003953   \n",
       "4                                       0.001342             0.000988   \n",
       "...                                          ...                  ...   \n",
       "4995                                    0.004027             0.004447   \n",
       "4996                                    0.001342             0.001482   \n",
       "4997                                    0.006711             0.008104   \n",
       "4998                                    0.004027             0.004447   \n",
       "4999                                    0.001342             0.001482   \n",
       "\n",
       "      responder pkts  responder ip bytes                      label  \\\n",
       "0           0.000000            0.000000  PartOfAHorizontalPortScan   \n",
       "1           0.000000            0.000000  PartOfAHorizontalPortScan   \n",
       "2           0.000000            0.000000  PartOfAHorizontalPortScan   \n",
       "3           0.000000            0.000000  PartOfAHorizontalPortScan   \n",
       "4           0.000000            0.000000  PartOfAHorizontalPortScan   \n",
       "...              ...                 ...                        ...   \n",
       "4995        0.000000            0.000000                        C&C   \n",
       "4996        0.000000            0.000000                        C&C   \n",
       "4997        0.002751            0.004626                        C&C   \n",
       "4998        0.000000            0.000000                        C&C   \n",
       "4999        0.000000            0.000000                        C&C   \n",
       "\n",
       "      protocol icmp  ...  connection state RSTOS0  connection state RSTR  \\\n",
       "0               0.0  ...                      0.0                    0.0   \n",
       "1               0.0  ...                      0.0                    0.0   \n",
       "2               0.0  ...                      0.0                    0.0   \n",
       "3               0.0  ...                      0.0                    0.0   \n",
       "4               0.0  ...                      0.0                    0.0   \n",
       "...             ...  ...                      ...                    ...   \n",
       "4995            0.0  ...                      0.0                    0.0   \n",
       "4996            0.0  ...                      0.0                    0.0   \n",
       "4997            0.0  ...                      0.0                    1.0   \n",
       "4998            0.0  ...                      0.0                    0.0   \n",
       "4999            0.0  ...                      0.0                    0.0   \n",
       "\n",
       "      connection state RSTRH  connection state S0  connection state S1  \\\n",
       "0                        0.0                  1.0                  0.0   \n",
       "1                        0.0                  1.0                  0.0   \n",
       "2                        0.0                  1.0                  0.0   \n",
       "3                        0.0                  1.0                  0.0   \n",
       "4                        0.0                  1.0                  0.0   \n",
       "...                      ...                  ...                  ...   \n",
       "4995                     0.0                  1.0                  0.0   \n",
       "4996                     0.0                  1.0                  0.0   \n",
       "4997                     0.0                  0.0                  0.0   \n",
       "4998                     0.0                  1.0                  0.0   \n",
       "4999                     0.0                  1.0                  0.0   \n",
       "\n",
       "      connection state S2  connection state S3  connection state SF  \\\n",
       "0                     0.0                  0.0                  0.0   \n",
       "1                     0.0                  0.0                  0.0   \n",
       "2                     0.0                  0.0                  0.0   \n",
       "3                     0.0                  0.0                  0.0   \n",
       "4                     0.0                  0.0                  0.0   \n",
       "...                   ...                  ...                  ...   \n",
       "4995                  0.0                  0.0                  0.0   \n",
       "4996                  0.0                  0.0                  0.0   \n",
       "4997                  0.0                  0.0                  0.0   \n",
       "4998                  0.0                  0.0                  0.0   \n",
       "4999                  0.0                  0.0                  0.0   \n",
       "\n",
       "      connection state SH  connection state SHR  \n",
       "0                     0.0                   0.0  \n",
       "1                     0.0                   0.0  \n",
       "2                     0.0                   0.0  \n",
       "3                     0.0                   0.0  \n",
       "4                     0.0                   0.0  \n",
       "...                   ...                   ...  \n",
       "4995                  0.0                   0.0  \n",
       "4996                  0.0                   0.0  \n",
       "4997                  0.0                   0.0  \n",
       "4998                  0.0                   0.0  \n",
       "4999                  0.0                   0.0  \n",
       "\n",
       "[5000 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize\n",
    "# columns_to_normalize = ['duration', 'payload bytes the originator sent', 'payload bytes the responder sent', 'missed bytes', 'number of packets that the originator sent', 'originator ip bytes', 'responder pkts', 'responder ip bytes', 'protocol icmp', 'protocol tcp', 'protocol udp', 'connection state OTH', 'connection state REJ', 'connection state RSTO', 'connection state RSTOS0', 'connection state RSTR', 'connection state RSTRH', 'connection state S0', 'connection state S1', 'connection state S2', 'connection state S3', 'connection state SF', 'connection state SH', 'connection state SHR'] #My label\n",
    "# scaler = MinMaxScaler()\n",
    "# sampled_df[columns_to_normalize] = scaler.fit_transform(sampled_df[columns_to_normalize])\n",
    "# sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Old method to divide data,\n",
    "# # Create a list to store the converted data\n",
    "# json_list = []\n",
    "\n",
    "# for index, row in sampled_df.iterrows():\n",
    "#     # X = row[['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'proto_icmp', 'proto_tcp', 'proto_udp', 'conn_state_OTH', 'conn_state_REJ', 'conn_state_RSTO', 'conn_state_RSTOS0', 'conn_state_RSTR', 'conn_state_RSTRH', 'conn_state_S0', 'conn_state_S1', 'conn_state_S2', 'conn_state_S3', 'conn_state_SF', 'conn_state_SH', 'conn_state_SHR']] #original label\n",
    "#     # X = row[['duration', 'payload bytes_the_originator_sent', 'payload_bytes_the_responder_sent', 'missed_bytes', 'number_of_packets_that_the_originator_sent', 'originator_ip_bytes', 'responder_pkts', 'responder_ip_bytes', 'protocol_icmp', 'protocol_tcp', 'protocol_udp', 'connection_state_OTH', 'connection_state_REJ', 'connection_state_RSTO', 'connection_state_RSTOS0', 'connection_state_RSTR', 'connection_state_RSTRH', 'connection_state_S0', 'connection_state_S1', 'connection_state_S2', 'connection_state_S3', 'connection_state_SF', 'connection_state_SH', 'connection_state_SHR']] #My label\n",
    "#     X = row[['duration', 'payload bytes the originator sent', 'payload bytes the responder sent', 'missed bytes', 'number of packets that the originator sent', 'originator ip bytes', 'responder pkts', 'responder ip bytes', 'protocol icmp', 'protocol tcp', 'protocol udp', 'connection state OTH', 'connection state REJ', 'connection state RSTO', 'connection state RSTOS0', 'connection state RSTR', 'connection state RSTRH', 'connection state S0', 'connection state S1', 'connection state S2', 'connection state S3', 'connection state SF', 'connection state SH', 'connection state SHR']] #My label\n",
    "#     Y = row['label']\n",
    "#     X_dict = X.to_dict()\n",
    "\n",
    "    \n",
    "\n",
    "#     # print(str(X_dict))\n",
    "#     # print(Y)\n",
    "#     # print(type(Y))\n",
    "\n",
    "#     json_object_gpt_turbo = {\"messages\": [{\"role\": \"system\", \"content\": \"You are a good network packet analyzer\\ Please analyze the network packet information provided by the user and determine its type\\ The possible types are 'PartOfAHorizontalPortScan', 'Okiru', 'Benign', 'DDoS', 'C&C'\\ Provide only the most likely type from these options as final output\\ Only output this type and nothing else\"}, {\"role\": \"user\", \"content\": prompt_template.format(information=str(X_dict))}, {\"role\": \"assistant\", \"content\":Y }]}\n",
    "#     json_object_babbage= {\"prompt\": prompt_template.format(information=str(X_dict)), \"completion\": Y}\n",
    "#     json_list.append(json_object_gpt_turbo)\n",
    "#     # print(json_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON objects have been split (80% \u0007nd 20%) and written to dataset_5000_train.jsonl and dataset_5000_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# # Old method to divide data, shuffle the jsonlist. Write the converted data to a JSON file\n",
    "# # output_file_path = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/dataset_babbage_5000.json\"  # Replace with the desired output file path\n",
    "\n",
    "\n",
    "# # Calculate the split index\n",
    "# split_index = int(0.8 * len(json_list))\n",
    "\n",
    "# # Split the list into two parts randomly \n",
    "# random.shuffle(json_list)   #打乱列表使得里面的数据随机\n",
    "# train_data = json_list[:split_index]\n",
    "# test_data = json_list[split_index:]\n",
    "\n",
    "# # Calculate the train dataset number and test dataset number\n",
    "# # train_data_number = int(0.8 * len(json_list))\n",
    "# # # Split the list into two parts randomly\n",
    "# # train_data= random.sample(json_list, train_data_number)\n",
    "# # # test_data = [item for item in json_list if item not in train_data]\n",
    "# # test_data = set(json_list) - set(train_data)\n",
    "\n",
    "\n",
    "# # File paths for train and test data\n",
    "# train_file_path = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label2_dataset_gpt_turbo_5000_train.jsonl\"\n",
    "# test_file_path = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label2_dataset_gpt_turbo_5000_test.jsonl\"\n",
    "# # Write train data to file\n",
    "# with open(train_file_path, \"w\") as train_file:\n",
    "#     for json_object in train_data:\n",
    "#         line = json.dumps(json_object) + \"\\n\"\n",
    "#         train_file.write(line)\n",
    "\n",
    "# # Write test data to file\n",
    "# with open(test_file_path, \"w\") as test_file:\n",
    "#     for json_object in test_data:\n",
    "#         line = json.dumps(json_object) + \"\\n\"\n",
    "#         test_file.write(line)\n",
    "\n",
    "# print(\n",
    "#     \"JSON objects have been split (80% \\and 20%) and written to dataset_5000_train.jsonl and dataset_5000_test.jsonl\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New method to divide data, keep the train data same\n",
    "\n",
    "df_shuffled = sampled_df.sample(frac=1)   #Shuffle this data set, frac=1 means keep all data (打乱这个数据集，frac=1表示保留所有数据)\n",
    "split_ratio = 0.8            # 80% of the data is used for training and 20% of the data is used for testing (80%的数据用于训练，20%的数据用于测试)\n",
    "split_index = int(split_ratio * len(df_shuffled))\n",
    "\n",
    "sampled_df_train_data = df_shuffled[:split_index]\n",
    "sampled_df_test_data = df_shuffled[split_index:]\n",
    "\n",
    "#store the train data and test data to diffent csv file\n",
    "sampled_df_train_data.to_csv(\"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment7_5000_train.csv\",index=False)\n",
    "sampled_df_test_data.to_csv(\"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment7_5000_test.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new method: create json list fot train and test data\n",
    "json_list_train_data = []\n",
    "json_list_test_data = []\n",
    "\n",
    "#生成train data的josn list\n",
    "for index, row in sampled_df_train_data.iterrows():\n",
    "    # X = row[['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'proto_icmp', 'proto_tcp', 'proto_udp', 'conn_state_OTH', 'conn_state_REJ', 'conn_state_RSTO', 'conn_state_RSTOS0', 'conn_state_RSTR', 'conn_state_RSTRH', 'conn_state_S0', 'conn_state_S1', 'conn_state_S2', 'conn_state_S3', 'conn_state_SF', 'conn_state_SH', 'conn_state_SHR']] #original label\n",
    "    # X = row[['duration', 'payload bytes_the_originator_sent', 'payload_bytes_the_responder_sent', 'missed_bytes', 'number_of_packets_that_the_originator_sent', 'originator_ip_bytes', 'responder_pkts', 'responder_ip_bytes', 'protocol_icmp', 'protocol_tcp', 'protocol_udp', 'connection_state_OTH', 'connection_state_REJ', 'connection_state_RSTO', 'connection_state_RSTOS0', 'connection_state_RSTR', 'connection_state_RSTRH', 'connection_state_S0', 'connection_state_S1', 'connection_state_S2', 'connection_state_S3', 'connection_state_SF', 'connection_state_SH', 'connection_state_SHR']] #My label\n",
    "    X = row[['duration', 'payload bytes the originator sent', 'payload bytes the responder sent', 'missed bytes', 'number of packets that the originator sent', 'originator ip bytes', 'responder pkts', 'responder ip bytes', 'protocol icmp', 'protocol tcp', 'protocol udp', 'connection state OTH', 'connection state REJ', 'connection state RSTO', 'connection state RSTOS0', 'connection state RSTR', 'connection state RSTRH', 'connection state S0', 'connection state S1', 'connection state S2', 'connection state S3', 'connection state SF', 'connection state SH', 'connection state SHR']] #My label\n",
    "    Y = row['label']\n",
    "    X_dict = X.to_dict()    \n",
    "\n",
    "    json_object_gpt_turbo = {\"messages\": [{\"role\": \"system\", \"content\": \"You are a good network packet analyzer\\ Please analyze the network packet information provided by the user and determine its type\\ The possible types are 'PartOfAHorizontalPortScan', 'Okiru', 'Benign', 'DDoS', 'C&C'\\ Provide only the most likely type from these options as final output\\ Only output this type and nothing else\\ Double-check your answer to ensure accuracy before outputting\"}, {\"role\": \"user\", \"content\": prompt_template.format(information=str(X_dict))}, {\"role\": \"assistant\", \"content\":Y }]}\n",
    "    json_object_babbage= {\"prompt\": prompt_template.format(information=str(X_dict)), \"completion\": Y}\n",
    "    json_list_train_data.append(json_object_gpt_turbo)\n",
    "    # print(json_list)\n",
    "\n",
    "#生成test data的josn list\n",
    "for index, row in sampled_df_test_data.iterrows():\n",
    "    # X = row[['duration', 'orig_bytes', 'resp_bytes', 'missed_bytes', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'proto_icmp', 'proto_tcp', 'proto_udp', 'conn_state_OTH', 'conn_state_REJ', 'conn_state_RSTO', 'conn_state_RSTOS0', 'conn_state_RSTR', 'conn_state_RSTRH', 'conn_state_S0', 'conn_state_S1', 'conn_state_S2', 'conn_state_S3', 'conn_state_SF', 'conn_state_SH', 'conn_state_SHR']] #original label\n",
    "    # X = row[['duration', 'payload bytes_the_originator_sent', 'payload_bytes_the_responder_sent', 'missed_bytes', 'number_of_packets_that_the_originator_sent', 'originator_ip_bytes', 'responder_pkts', 'responder_ip_bytes', 'protocol_icmp', 'protocol_tcp', 'protocol_udp', 'connection_state_OTH', 'connection_state_REJ', 'connection_state_RSTO', 'connection_state_RSTOS0', 'connection_state_RSTR', 'connection_state_RSTRH', 'connection_state_S0', 'connection_state_S1', 'connection_state_S2', 'connection_state_S3', 'connection_state_SF', 'connection_state_SH', 'connection_state_SHR']] #My label\n",
    "    X = row[['duration', 'payload bytes the originator sent', 'payload bytes the responder sent', 'missed bytes', 'number of packets that the originator sent', 'originator ip bytes', 'responder pkts', 'responder ip bytes', 'protocol icmp', 'protocol tcp', 'protocol udp', 'connection state OTH', 'connection state REJ', 'connection state RSTO', 'connection state RSTOS0', 'connection state RSTR', 'connection state RSTRH', 'connection state S0', 'connection state S1', 'connection state S2', 'connection state S3', 'connection state SF', 'connection state SH', 'connection state SHR']] #My label\n",
    "    Y = row['label']\n",
    "    X_dict = X.to_dict()    \n",
    "\n",
    "    json_object_gpt_turbo = {\"messages\": [{\"role\": \"system\", \"content\": \"You are a good network packet analyzer\\ Please analyze the network packet information provided by the user and determine its type\\ The possible types are 'PartOfAHorizontalPortScan', 'Okiru', 'Benign', 'DDoS', 'C&C'\\ Provide only the most likely type from these options as final output\\ Only output this type and nothing else\\ Double-check your answer to ensure accuracy before outputting \"}, {\"role\": \"user\", \"content\": prompt_template.format(information=str(X_dict))}, {\"role\": \"assistant\", \"content\":Y }]}\n",
    "    json_object_babbage= {\"prompt\": prompt_template.format(information=str(X_dict)), \"completion\": Y}\n",
    "    json_list_test_data.append(json_object_gpt_turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON objects have been split (80% \u0007nd 20%) and written to dataset_5000_train.jsonl and dataset_5000_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# new method: store train and test data to jsonal file\n",
    "# File paths for train and test data\n",
    "train_file_path = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label2_dataset_gpt_turbo_10000_train_Experiment5.jsonl\"\n",
    "test_file_path = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label2_dataset_gpt_turbo_10000_test_Experiment5.jsonl\"\n",
    "# Write train data to file\n",
    "with open(train_file_path, \"w\") as train_file:\n",
    "    for json_object in json_list_train_data:\n",
    "        line = json.dumps(json_object) + \"\\n\"\n",
    "        train_file.write(line)\n",
    "\n",
    "# Write test data to file\n",
    "with open(test_file_path, \"w\") as test_file:\n",
    "    for json_object in json_list_test_data:\n",
    "        line = json.dumps(json_object) + \"\\n\"\n",
    "        test_file.write(line)\n",
    "\n",
    "print(\n",
    "    \"JSON objects have been split (80% \\and 20%) and written to dataset_5000_train.jsonl and dataset_5000_test.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Step2.1: Upload the training dataset to openAI</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-C8XS5VX5bsZXot93JtwS6cBy', bytes=2645762, created_at=1713965965, filename='new_label2_dataset_gpt_turbo_10000_test_Experiment5.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='**********')\n",
    "\n",
    "\n",
    "# Path to training data\n",
    "training_dataset = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label2_dataset_gpt_turbo_10000_train_Experiment5.jsonl\"\n",
    "test_dataset= \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label2_dataset_gpt_turbo_10000_test_Experiment5.jsonl\"\n",
    "# Upload train file to openai\n",
    "client.files.create(\n",
    "  # file=open(test_dataset, \"rb\"),\n",
    "  file=open(training_dataset, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# Upload test file to openai\n",
    "client.files.create(\n",
    "  # file=open(test_dataset, \"rb\"),\n",
    "  file=open(test_dataset, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "\n",
    "# 1. FileObject(id='file-cGUYmAcGEA1yald9ybSFDBzX', bytes=159095, created_at=1706025370, filename='dataset_1000_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
    "#babbage:FileObject(id='file-OwXvvH4pDQ5ERzZDrASTWWFF', bytes=133495, created_at=1706030310, filename='dataset_babbage_1000_test.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
    "#babbage test:file-Digcrly10euFMAj9VdYRByHC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Step2.2: Create a finetone job</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-Qq3oFe9gf1cQCcBEvwy7LVkE', created_at=1706030370, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='babbage-002', object='fine_tuning.job', organization_id='org-gS5nata0XGKHO0BeClpnTQny', result_files=[], status='validating_files', trained_tokens=None, training_file='file-OwXvvH4pDQ5ERzZDrASTWWFF', validation_file=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='**********')\n",
    "\n",
    "\n",
    "# check file id: print(openai.File.list())\n",
    "file_id = \"**********\"\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "  training_file=file_id, \n",
    "  model=\"babbage-002\"\n",
    ")\n",
    "\n",
    "#chatgpt3.5: FineTuningJob(id='ftjob-PRMI4gtCcxXgXfHhU0QktlEv', created_at=1706025417, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-1106', object='fine_tuning.job', organization_id='org-gS5nata0XGKHO0BeClpnTQny', result_files=[], status='validating_files', trained_tokens=None, training_file='file-cGUYmAcGEA1yald9ybSFDBzX', validation_file=None)\n",
    "#babbage: FineTuningJob(id='ftjob-Qq3oFe9gf1cQCcBEvwy7LVkE', created_at=1706030370, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='babbage-002', object='fine_tuning.job', organization_id='org-gS5nata0XGKHO0BeClpnTQny', result_files=[], status='validating_files', trained_tokens=None, training_file='file-OwXvvH4pDQ5ERzZDrASTWWFF', validation_file=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Step2.3: Check fine-tune job info</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-GJx5k8DUVM2kGpe4uxUi5Pgy', created_at=1706046133, error=None, fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='babbage-002', object='fine_tuning.job', organization_id='org-gS5nata0XGKHO0BeClpnTQny', result_files=[], status='cancelled', trained_tokens=None, training_file='file-OwXvvH4pDQ5ERzZDrASTWWFF', validation_file='file-Digcrly10euFMAj9VdYRByHC')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key='************')\n",
    "# List fine-tuning jobs\n",
    "client.fine_tuning.jobs.list()\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "# client.fine_tuning.jobs.retrieve(\"ftjob-Qq3oFe9gf1cQCcBEvwy7LVkE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Step3: Try the fine-tuned model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_information= \"{'duration': 2.999077, 'orig_bytes': 0, 'resp_bytes': 0, 'missed_bytes': 0.0, 'orig_pkts': 3.0, 'orig_ip_bytes': 180.0, 'resp_pkts': 0.0, 'resp_ip_bytes': 0.0, 'proto_icmp': False, 'proto_tcp': True, 'proto_udp': False, 'conn_state_OTH': False, 'conn_state_REJ': False, 'conn_state_RSTO': False, 'conn_state_RSTOS0': False, 'conn_state_RSTR': False, 'conn_state_RSTRH': False, 'conn_state_S0': True, 'conn_state_S1': False, 'conn_state_S2': False, 'conn_state_S3': False, 'conn_state_SF': False, 'conn_state_SH': False, 'conn_state_SHR': False}\" #PartOfAHorizontalPortScan\n",
    "test_information2=\"{'duration': 0.0, 'orig_bytes': 0, 'resp_bytes': 0, 'missed_bytes': 0.0, 'orig_pkts': 1.0, 'orig_ip_bytes': 40.0, 'resp_pkts': 0.0, 'resp_ip_bytes': 0.0, 'proto_icmp': False, 'proto_tcp': False, 'proto_udp': True, 'conn_state_OTH': False, 'conn_state_REJ': False, 'conn_state_RSTO': False, 'conn_state_RSTOS0': False, 'conn_state_RSTR': False, 'conn_state_RSTRH': False, 'conn_state_S0': True, 'conn_state_S1': False, 'conn_state_S2': False, 'conn_state_S3': False, 'conn_state_SF': False, 'conn_state_SH': False, 'conn_state_SHR': False}\" #benign\n",
    "test_information3= \"{'duration': 0.0, 'orig_bytes': 0, 'resp_bytes': 0, 'missed_bytes': 0.0, 'orig_pkts': 1.0, 'orig_ip_bytes': 60.0, 'resp_pkts': 0.0, 'resp_ip_bytes': 0.0, 'proto_icmp': False, 'proto_tcp': True, 'proto_udp': False, 'conn_state_OTH': False, 'conn_state_REJ': False, 'conn_state_RSTO': False, 'conn_state_RSTOS0': False, 'conn_state_RSTR': False, 'conn_state_RSTRH': False, 'conn_state_S0': True, 'conn_state_S1': False, 'conn_state_S2': False, 'conn_state_S3': False, 'conn_state_SF': False, 'conn_state_SH': False, 'conn_state_SHR': False}\" #PartOfAHorizontalPortScan\n",
    "test_information4= \"{'duration': 0.0, 'orig_bytes': 0, 'resp_bytes': 0, 'missed_bytes': 0.0, 'orig_pkts': 1.0, 'orig_ip_bytes': 40.0, 'resp_pkts': 0.0, 'resp_ip_bytes': 0.0, 'proto_icmp': False, 'proto_tcp': False, 'proto_udp': True, 'conn_state_OTH': False, 'conn_state_REJ': False, 'conn_state_RSTO': False, 'conn_state_RSTOS0': False, 'conn_state_RSTR': False, 'conn_state_RSTRH': False, 'conn_state_S0': True, 'conn_state_S1': False, 'conn_state_S2': False, 'conn_state_S3': False, 'conn_state_SF': False, 'conn_state_SH': False, 'conn_state_SHR': False}\" #benign\n",
    "test_information5=\"{'duration': 2.998829, 'orig_bytes': 0, 'resp_bytes': 0, 'missed_bytes': 0.0, 'orig_pkts': 3.0, 'orig_ip_bytes': 180.0, 'resp_pkts': 0.0, 'resp_ip_bytes': 0.0, 'proto_icmp': False, 'proto_tcp': True, 'proto_udp': False, 'conn_state_OTH': False, 'conn_state_REJ': False, 'conn_state_RSTO': False, 'conn_state_RSTOS0': False, 'conn_state_RSTR': False, 'conn_state_RSTRH': False, 'conn_state_S0': True, 'conn_state_S1': False, 'conn_state_S2': False, 'conn_state_S3': False, 'conn_state_SF': False, 'conn_state_SH': False, 'conn_state_SHR': False}\" #benign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test finntuned gpt3.5 model\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key='***********')\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal:5k-newlabel:9HCxFiMg\",  #finetune model name\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a good network package analyzer\\ Please analyze the network packet information provided by the user and determine its type\\ The possible types are 'PartOfAHorizontalPortScan', 'Okiru', 'Benign', 'DDoS', 'C&C'\\ Provide only the most likely type from these options as final output\\ Only output this type and nothing else\"},\n",
    "    {\"role\": \"user\", \"content\": prompt_template.format(information=test_information)}\n",
    "  ]   \n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test funetune babbage model\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key='**********')\n",
    "\n",
    "completion = client.completions.create(model='ft:gpt-3.5-turbo-0125:personal:turbo0125-1000:8zRrOLmx', prompt=prompt_template.format(information=str(test_information2)))\n",
    "print(completion.choices[0].text)\n",
    "# print(dict(completion).get('usage'))\n",
    "# print(completion.model_dump_json(indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Step4: Evaluate the fine-tuned model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the baggage model\n",
    "import json\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "def evaluate_finetuned_model(test_file,result_file):\n",
    "    client = OpenAI(api_key='*************')\n",
    "    finetuned_model_id=\"*************\"\n",
    "    total_rows = 0\n",
    "    correct_predicition=0\n",
    "    error_predicition=0\n",
    "    \n",
    "    # completion = client.completions.create(model='ft:babbage-002:personal::8p5AswsM', prompt=prompt_template.format(information=str(test_information4)))\n",
    "    with open(result_file, 'w', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['original_label', 'model_prediction', 'whether_first_string_of_output_match_label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        with open(test_file, 'r') as file:\n",
    "            for line in file:\n",
    "                total_rows+=1\n",
    "                # 解析json数据\n",
    "                data = json.loads(line)\n",
    "                # 输出prompt和completion\n",
    "                network_information = data['prompt']   \n",
    "                label = data['completion']\n",
    "                #用finetuned model 进行分析\n",
    "                completion = client.completions.create(model=finetuned_model_id, prompt=prompt_template.format(information=str(network_information)))\n",
    "                model_prediction=completion.choices[0].text     #记录下模型的completion的text    \n",
    "                #判断output中是否有真实label\n",
    "                # if(label in model_prediction):\n",
    "                #     correct_predicition+=1\n",
    "                # else:\n",
    "                #     error_predicition+=1\n",
    "                #判断output是否以label开头\n",
    "                if model_prediction.startswith(label):\n",
    "                    whether_first_string_of_output_match_label=True\n",
    "                    correct_predicition+=1\n",
    "                else:\n",
    "                    whether_first_string_of_output_match_label=False\n",
    "                    error_predicition+=1\n",
    "\n",
    "                writer.writerow({\n",
    "                    'original_label': label,\n",
    "                    \n",
    "                    'model_prediction': model_prediction,\n",
    "                    'whether_first_string_of_output_match_label':whether_first_string_of_output_match_label\n",
    "                })\n",
    "    accurency=correct_predicition/total_rows\n",
    "    print(f\"The correct prediction number is {correct_predicition}\")\n",
    "    print(f\"The error prediction number is {error_predicition}\")\n",
    "    print(f\"The accurency of this model is {accurency}\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/new_label_dataset_babbage_10000_test.jsonl\"\n",
    "result_file = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Result_Files/new_label_result_davinvi_10000.csv\"\n",
    "evaluate_finetuned_model(test_file,result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate gpt3.5 model\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "def evaluate_finetuned_gpt_model(test_file,result_file):\n",
    "    client = OpenAI(api_key='*************')\n",
    "    finetuned_model_id=\"*************\"\n",
    "    base_model= \"gpt-3.5-turbo\"\n",
    "    total_rows = 0\n",
    "    correct_predicition=0\n",
    "    error_predicition=0\n",
    "    y_true=[]\n",
    "    y_pred=[]\n",
    "    \n",
    "    # completion = client.completions.create(model='ft:babbage-002:personal::8p5AswsM', prompt=prompt_template.format(information=str(test_information4)))\n",
    "    with open(result_file, 'w', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['original_label', 'model_prediction', 'whether_first_string_of_output_match_label']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        with open(test_file, 'r') as file:\n",
    "            for line in file:\n",
    "                total_rows+=1\n",
    "                # 解析json数据\n",
    "                data = json.loads(line)\n",
    "                # 输出改行的message中的user和assistant的content\n",
    "                user_content = None\n",
    "                assistant_content = None\n",
    "                for message in data['messages']:\n",
    "                    role = message['role']\n",
    "                    content = message['content']\n",
    "                    # 根据role提取对应的内容\n",
    "                    if role == 'user':\n",
    "                        user_content = content\n",
    "                    elif role == 'assistant':\n",
    "                        assistant_content = content\n",
    "                \n",
    "                #用finetuned model 进行分析\n",
    "                response = client.chat.completions.create(\n",
    "                    model=base_model,  #finetune model name\n",
    "                    temperature=0,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a good network package analyzer\\ Please analyze the network packet information provided by the user and determine its type\\ The possible types are 'PartOfAHorizontalPortScan', 'Okiru', 'Benign', 'DDoS', 'C&C'\\ Provide only the most likely type from these options as final output\\ Only output this type and nothing else\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt_template.format(information=user_content)}\n",
    "                    ]   \n",
    "                    ) \n",
    "                model_prediction=response.choices[0].message.content     #记录下模型的response的content    \n",
    "                #判断output中是否有真实label\n",
    "                # if(label in model_prediction):\n",
    "                #     correct_predicition+=1\n",
    "                # else:\n",
    "                #     error_predicition+=1\n",
    "                #判断output是否以label开头\n",
    "                if model_prediction.startswith(assistant_content):   #判断finetuned后的模型的输出是否以原本正确的label开头\n",
    "                    whether_first_string_of_output_match_label=True\n",
    "                    correct_predicition+=1\n",
    "                else:\n",
    "                    whether_first_string_of_output_match_label=False\n",
    "                    error_predicition+=1\n",
    "                y_true.append(assistant_content)\n",
    "                y_pred.append(model_prediction)\n",
    "                writer.writerow({\n",
    "                    'original_label': assistant_content,\n",
    "                    \n",
    "                    'model_prediction': model_prediction,\n",
    "                    'whether_first_string_of_output_match_label':whether_first_string_of_output_match_label\n",
    "                })\n",
    "    accurency=correct_predicition/total_rows\n",
    "    confusion_mat = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    report = classification_report(y_true, y_pred, digits=3) \n",
    "    print(f\"The correct prediction number is {correct_predicition}\")\n",
    "    print(f\"The error prediction number is {error_predicition}\")\n",
    "    print(f\"The accurency of this model is {accurency}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mat)\n",
    "    print(\"Classifiction Report :\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct prediction number is 228\n",
      "The error prediction number is 772\n",
      "The accurency of this model is 0.228\n",
      "Confusion Matrix:\n",
      "[[ 45   2   3   0 154]\n",
      " [  1   0  33   0 167]\n",
      " [107   0   0   0  92]\n",
      " [  0   0   0   0 198]\n",
      " [ 10   3   2   0 183]]\n",
      "Classifiction Report :\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                   Benign      0.276     0.221     0.245       204\n",
      "                      C&C      0.000     0.000     0.000       201\n",
      "                     DDoS      0.000     0.000     0.000       199\n",
      "                    Okiru      0.000     0.000     0.000       198\n",
      "PartOfAHorizontalPortScan      0.230     0.924     0.369       198\n",
      "\n",
      "                 accuracy                          0.228      1000\n",
      "                macro avg      0.101     0.229     0.123      1000\n",
      "             weighted avg      0.102     0.228     0.123      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hzhou/anaconda3/envs/GPU3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hzhou/anaconda3/envs/GPU3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hzhou/anaconda3/envs/GPU3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_file = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/Experiment8_test_data_10.jsonl\"\n",
    "result_file = \"/home/hzhou/LLM_Encrypted_Traffic_Analysis/Anomaly-Detection-IoT23-main/Data Preprocessing/Experiment8/result/Experiment8_basemodel_3.5_result10.csv\"\n",
    "evaluate_finetuned_gpt_model(test_file,result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
